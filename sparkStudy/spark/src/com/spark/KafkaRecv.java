package com.spark;

import java.util.Arrays;
import java.util.Collections;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;
import java.util.Set;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaPairInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka.KafkaUtils;

import kafka.serializer.StringDecoder;
import redis.clients.jedis.Jedis;
import redis.clients.jedis.JedisPool;
import redis.clients.jedis.JedisPoolConfig;
import scala.Tuple2;

//kafka의 서버에서  값을 받음
public class KafkaRecv {

	public static void main(String[] args) {
			SparkConf conf = new SparkConf()
					.setMaster("local[*]")
					.setAppName("KafkaRecv");
			JavaSparkContext sc = new JavaSparkContext(conf);
			JavaStreamingContext ssc 
				= new JavaStreamingContext(sc, Durations.seconds(3));
			System.out.println(ssc);
			

			//토픽 설정
			///Singleton: Returns an immutable set containing only the specified object.The returned set is serializable.
			Set<String> topic = Collections.singleton("testPNU");
			Map<String, String> params = new HashMap<String, String>();
			//Check KafkaSend.java --- Put 'bootstrap.servers'
			//map or set 또는 여러가지가 있기에 넣어 주는 것들이 다르다
			params.put("metadata.broker.list", "192.168.99.102:9092"); 
			
			JavaPairInputDStream<String, String> stream 
				= KafkaUtils.createDirectStream(
						ssc, //streamingContext object
						String.class, //keyClass - class of the keys in the Kafka records
						String.class, //valueClass - class of the values in the Kafka records
						StringDecoder.class, //keyDecoderClass - class of the key decoder
						StringDecoder.class, //valueDecoderClass - class of the value decoder
						// Kafka configuration parameters , Requires "metadata.broker.list" or "bootstrap.servers" 
						// to be set with Kafka broker(s) (NOT zookeeper servers), specified in the port
						params, 
						topic);
			
			/*
			 * A Java-friendly interface to DStream, the basic abstraction in Spark Streaming 
			 * that represents a continuous stream of data. 
			 * DStreams can either be created from live data 
			 * (such as, data from TCP sockets, Kafka, Flume, etc.) 
			 * or it can be generated by transforming existing DStreams 
			 * using operations such as map, window. 
			 * For operations applicable to key-value pair DStreams, see JavaPairDStream.
			 */
			JavaDStream<String> lines = stream.map(
	        		new Function<Tuple2<String, String>, String>() {
		        @Override
		        public String call(Tuple2<String, String> tuple2) {
		        	//_ is for placeholder of anonymous function
		        	//_2 is a member of case class Tuple
		        	return tuple2._2();
		        }
	        });
	        //추가된 부분=======================================
	        JavaDStream<String> words = lines.flatMap(
	        		new FlatMapFunction<String, String>(){
	    		@Override
	    		public Iterator<String> call(String t) throws Exception {
	    			return Arrays.asList(t.split(" ")).iterator();
	    		}
	        });
	        
	        JavaPairDStream<String, Integer> pairs = words.mapToPair(
	        		new PairFunction<String,String, Integer>(){
	    		@Override
	    		public Tuple2<String, Integer> call(String t) throws Exception {
	    			return new Tuple2<String, Integer>(t,1);
	    		}
	        });

			JavaPairDStream<String, Integer> wordcount = pairs.reduceByKey(
					new Function2<Integer, Integer, Integer>(){
				@Override
				public Integer call(Integer v1, Integer v2) throws Exception {
					return v1+v2;
				}
			});
			
			wordcount.print();
			
//			stream.foreachRDD(rdd-> {
//				rdd.foreach(record->{
//					String ans = record._2;
//					System.out.println(ans);
//					processFunc(ans);
//					//redis에 서버에 값을 넣음
//				});
//			});
			ssc.start();
			ssc.awaitTermination();
	}
	
	//Add value into Redis
	public static void processFunc(String st) {
		String host = "127.0.0.1";
		int port = 6379;
		int timeout = 3000;
		int db = 0;

		JedisPoolConfig jedisPoolConfig = new JedisPoolConfig();
		JedisPool pool = 
				new JedisPool(
						jedisPoolConfig, 
						host, 
						port, 
						timeout, 
						null, db); //null 비밀번호 설정 
		Jedis jedis = pool.getResource();
		// Connect 체크
		//System.out.println(jedis.isConnected());

		jedis.set("key2", st);
		System.out.println("Redis value: "+ jedis.get("key2"));
		if (jedis != null) {
			jedis.close();
		}
		pool.close();
	}
		
}
